# :memo: 机器学习概述
1. [机器学习的定义](#1-机器学习定义)
2. [Why机器学习](#2-为什么要使用机器学习)
    + [传统编程](#21-传统编程)
    + [机器学习编程](#22-机器学习解决方法)
    + [机器学习优点](#23-机器学习的优点)
3. [机器学习系统的种类](#3-ml系统的类型)
    + [监督/无监督式学习](#31-监督式无监督式学习)
        * [监督学习](#311-监督式学习supervised-learning)
        * [无监督学习](#312-无监督式学习unsupervised-learning)
        * [半监督学习](#313-半监督式学习semisupervised-learning)
        * [强化学习](#314-强化学习reinforcement-learning-rl)
    + [批量和在线学习](#32-批量学习和在线学习)
    + [基于实例的学习和基于模型的学习](#33-基于实例的学习和基于模型的学习)
4. [机器学习的主要挑战](#4-机器学习的主要挑战)
    + [“坏数据”](#41-坏数据)
        * [训练数据量不足](#411-训练数据量不足)
        * [训练数据不具代表性](#412-训练数据不具代表性)
        * [数据质量差](#413-数据质量差)
        * [无关特征](#414-无关特征)
    + [“坏算法”](#42-坏算法)
        * [过拟合](#421-过拟合overfitting)
        * [欠拟合](#422-欠拟合underfitting)
5. [测试与验证](#5-测试与验证)
## 1 机器学习定义

机器学习是一门能够让变成计算机从*数据*中学习的计算机科学（和艺术）。

+ **一个笼统的定义：**
机器学习研究如何让计算机不需要明确的程序也具备学习能力。

+ **偏工程化的定义：**
一个计算机程序在完成任务T之后，获得经验E，其表现效果为P，如果任务T的性能表现P，随着E的增加而增加，可以称其为*学习*。

+ **机器学习技术定义：**
在预先定义好的可能性空间（假设空间）中，利用反馈信号的指引来寻找输入数据的有用表示。

+ **深度学习技术定义：**
学习数据表示的多级方法。

> 后面两种定义来自[《Deep Learning with Python》](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_3?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&keywords=deep+learning&qid=1555341849&s=gateway&sr=8-3)

## 2 为什么要使用机器学习
![机器学习编程](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/ml-coding.png)

### 2.1 传统编程

输入的是规则（即程序）和需要根据这些规则进行处理的数据，系统的输出是答案。
![传统编程方法](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/traditional-approach.png)

程序很可能变成一长串的复杂规则，很难维护；传统方法太过复杂或者甚至不存在已知算法。

### 2.2 机器学习解决方法

输入的数据和从这些数据预期得到的答案，系统输出的是规则。这些规则随后可应用于新的数据，并使计算机自主生成答案。

![机器学习解决方法](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/machine-learning-approach.png)

这样的程序简单的多，易于维护，并且可能还更准确。

而且还能自动适应变化，不需要人为干预。

![机器学习方法自适应变化](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/automatically-adapting-to-change.png)

机器学习可以帮助人类学习（如下图）：通过检视机器学习算法以了解它们学到了什么（有些算法是黑匣子，并不行）。

![机器学习能够帮助人类学习](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/machine-learning-can-help-humans-learn.png)

应用机器学习技术来挖掘海量数据，可以帮助我们发现那些此前并非立见端倪的模式，这个过程成为**数据挖掘(Data Mining)**。


### 2.3 机器学习的优点
+ 通过ML方法可以简化代码，并且提升执行表现——现有解决方案需要大量手动调整或者规则列表超长。
+ 一些复杂问题利用传统手段根本无法解决，而ML技术可以找到一个解决方案。
+ 对于环境波动：ML系统可以适应新的数据。
+ 从复杂问题和海量数据中获得洞见（get insights）。

## 3 ML系统的类型

**可以将ML系统进行如下大的分类：**
+ 是否在人类监督下训练（监督、无监督、半监督和强化学习）
+ 是否可以动态地进行增量学习（在线学习和批量学习）
+ 是简单的将新的数据点和已知的数据点进行匹配，还是建立一个预测模型（基于实例的学习和基于模型的学习）

这些分类方法之间并互斥，可以以任何的方式组合，如建立一个在线的、基于模型的、监督式学习系统。

### 3.1 监督式/无监督式学习

根据训练期接受的监督数量和监督种类。

#### 3.1.1 监督式学习（Supervised Learning）

提供给算法的训练数据包含*labels*。

**一些重要的监督式学习算法:**
+ K-means++算法
+ Linear Regression
+ Logistic Regression
+ Support Vector Machines, SVM
+ Decision Trees and Random Forests
+ Neural Networks

#### 3.1.2 无监督式学习（Unsupervised Learning）

训练数据都是未经标记的。

**一些重要的无监督式学习算法：**
1. **Clustering**
  + K-Means
  + 分层聚类分析（Hierarchical Cluster Analysis, HCA）
  + 最大期望算法（Expectation Maximization）
2. **Visualization and dimensionality reduction**
  + PCA
  + Kernel PCA
  + 局部线性嵌入（Locally Linear Embedding, LLE）
  + t-distributed Stochastic Neighbor Embedding, t-SNE
3. **Association rule learning 关联规则学习**
  + Apriori
  + Eclat


*可视化算法*会尽可能地保留尽量多的结构。

*降维*的目的是在不丢失太多信息的前提下简化数据。方法之一是将多个相关特征合并为一个。

**通常比较好的做法是，先使用降维算法减少训练数据的维度，再将其提供给另一个ML算法。这样会运行得更快，数据占用的磁盘空间和内存都会更小，在某些情况下，执行算法也会更好。**

*异常检测*：系统用正常实例进行训练，然后当看到新的实例时，它就可以判断出这个新实例看上去是正常还是异常。

*关联规则学习*目的是挖掘大量数据，发现属性之间的有趣联系。

#### 3.1.3 半监督式学习（Semisupervised Learning）

有些算法可以处理部分标记的训练数据——通常是大量未标记数据和少量的已标记数据。

大多数半监督式学习算法是无监督式和监督式算法的结合。如*深度信念网络(Deep Belief Networkds, DBNs)*基于一种互相堆叠的无监督式组件，这个组件称为*受限玻尔兹曼机(Restricted Boltzmann Machines, RBMS)*。RBM以无监督的方式进行训练，然后使用监督式学习对整个系统进行微调。

#### 3.1.4 强化学习（Reinforcement Learning, RL） 

RL的学习系统（称为*智能agent*）能够观察环境，做出选择，执行操作，并获得回报reward，或者是以负面回报的方式获得惩罚。必须自己学习什么是好的策略（policy），从而随着时间推移获得最大的回报。策略代表agent在特定情况下应该选择的操作。

![RL](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/Reinforcement-learning.png)

### 3.2 批量学习和在线学习

看看系统是否可以从传入的数据流中进行增量学习。

#### 3.2.1 Batch Learning

在批量学习中，系统无法进行增量学习——即必须使用所有可用的数据进行训练。消耗大量时间和计算资源，因此通常情况下都是*离线*完成的。

**先训练->停止训练、投入使用->停止使用->加入新的数据->训练得到一个新的系统->投入使用...**

每次使用全套数据集进行训练可能会花上好几个小时，这对于快速变化的任务（如预测股票价格），那么需要一个更好的方案。

而且，使用完整数据集训练需要耗费大量的计算资源（CPU、内存空间、磁盘空间、磁盘I/O、网络I/O等）；如果每天从0自动执行训练，花费大量金钱；如果面对海量数据，甚至不能用批量学习。

#### 3.2.2 Online Learning

循序渐进地给系统提供训练数据，逐步积累学习成果。提供数据的方式可以是单独的，也可以采用小批量，每一步学习都很快速且便宜。

适用范围：
- 需要接收持续的数据流（如，股票价格）的同时对数据流的变化进行快速反映；
- 计算资源有限——新的数据实例一旦经过系统的学习，就不再需要，可以丢弃（除非要回滚到之前的状态，需要重新学习）；
- 超大数据集（超出一台计算机的主存储器的数据），OL也适用，这称为**核外学习(out-of-core learning)**。算法每次只加载部分数据，并针对这部分数据进行训练，然后不断重复这个过程，直到完成所有数据的训练。

![online learning](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/online-learning.png)

OL的一个**重要参数**是其适应不断变化的数据的速度，即**学习率**。学习过大，系统会迅速适应新数据，同时也会很快忘记旧数据；学习率很低，系统学习缓慢，同时也对数据中的噪声或非典型数据点的序列更不敏感。

**OL的重大挑战：如果给系统输入不良数据，系统的性能将会逐渐下降**。需要密切监控系统，一旦检测到系统性能下降，要及时中断学习，可能还要恢复到之前的工作状态。同时，还要监控输入数据，并对异常数据做出反映（异常检测算法）。

### 3.3 基于实例的学习和基于模型的学习
主要看模型如何泛化。
#### 3.3.1 基于实例的学习
系统先*完全记住实例*（example），然后通过某种*相似性度量*方式将其泛化到新的实例。

![基于实例的学习flowchart](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/instance-based-learning.png)

#### 3.3.2 基于模型的学习
构建实例的模型，然后使用该模型进行预测。

![基于模型的学习](https://raw.githubusercontent.com/libingallin/hands-on-ml/master/cn/figures/ch1-figs/model-based-learning.png)

## 4 机器学习的主要挑战
由于机器学习主要任务是选择一种学习算法，并对某些数据进行训练，所以最可能出现的两个问题不外乎是*“坏算法”*和*“坏数据”*。
### 4.1 “坏数据”
#### 4.1.1 训练数据量不足
大部分机器学习算法需要大量的数据才能正常工作。对于复杂的问题（如图像和语音识别等），则可能需要更多的数据（除非重用现有模型的某些部分）。

论文（当然不是辣鸡论文）表示当数据量足够大时，各个算法的性能差距非常小，因此，需要在算法和获得数据之间做权衡。然而，现实中，中小型数据集依然非常普遍，获得额外的训练数据并不总是一件轻而易举的事情，所以暂时先不要抛弃算法。

#### 4.1.2 训练数据不具代表性
机器学习算法最终都要进行*泛化*，针对将要泛化的数据来说，训练数据一定要非常具有代表性，不论使用的基于实例的学习还是基于模型的学习。使用不具代表性的训练数据训练出来的模型不可能做出准确的预测。

如果样本集太小，将会出现采样噪声（即非代表性数据被选中）；即便是非常大的样本数据，如果采样方式欠妥，同样也可能导致非代表性数据被选中，这是所谓的**采样偏差**。

#### 4.1.3 数据质量差
如果训练数据集满是错误、异常值和噪声（如，差质量的测量产生的数据），系统将更难检测到底层模式，更不太可能表现良好。因此，需要花很大一部分时间来清理数据，这也非常值得。
+ 如果某些数据明显是错误的，要么直接丢弃，要么尝试手动修复错误，等。
+ 如果某些数据缺少部分特征，需要忽略这些特征，或者忽略这些数据，或者将缺失值填补完整，或者训练一个带这个特征的模型和一个不带这个特征的模型。

#### 4.1.4 无关特征
只有训练数据里包含足够多的相关特征，以及较少的无关特征，系统才能够完成学习。一个成功的机器项目中，关键部分是提取出一组好的用来训练的特征集，这个工程是**特征工程**，包括以下几点：
+ **特征选择**：从现有特征中选取最有用的特征进行训练。
+ **特征起去**：将现有的特征进行整合，产生更有用的特征。
+ 通过收集新数据创造新特征。

### 4.2 “坏算法”
#### 4.2.1 过拟合（Overfitting）
过拟合即训练数据拟合过度。诸如NN这类复杂模型可以检测到数据中的微小模式，如果训练集本身是嘈杂的，或者说数据集太小（会导致采样噪声），那么很可能会导致模型学到的是噪声里的模式。

当模型相对于训练数据的数量和噪声都过于复杂时，会发生过拟合。**解决过拟合的方法**:
+ 简化模型：可以使用较小参数的模型（如线性而非高阶多项式），可以减少训练数据中的属性数量，又或者约束模型（正则化）。
+ 获得更多的数据。
+ 减少训练数据中的噪声（如，修复错误和消除异常值）
> 在学习时，应用正则化的程度可以用过一个**超参数**来控制。超参数是学习算法（不是模型）的参数。因此，不受算法本身的影响；必须在训练之前设置好，并在训练期间保持不变。
#### 4.2.2 欠拟合（Underfitting）
欠拟合即训练数据拟合不足。通常是因为，对于下层的数据结构来说，使用的模型太简单了。**解决欠拟合的方法**:
+ 选择一个带有更多参数、更强大的模型。
+ 给学习算法提供更好的特征集（特征工程）
+ 减少模型中约束条件（如，较小正则化强度）

## 5 测试与验证
**数据集按80%~20%分为训练集和测试集**，同时为避免“浪费”太多的训练数据，常见的技术就是使用交叉验证来创建验证集。**一旦模型和超参数被选定，最终的模型会带着这些超参数对整个训练集进行一个训练，最后再用测试集测量泛化误差**。
